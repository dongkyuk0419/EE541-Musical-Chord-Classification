{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from scipy.io import wavfile\n",
    "import numpy as np\n",
    "nTimeStep = 100000\n",
    "\n",
    "MajorData = []\n",
    "for subdir, dirs, files in os.walk(r'./Dataset/Audio_Files/Major/'):\n",
    "    for file in files:\n",
    "        filepath = subdir + os.sep + file\n",
    "        samplerate, data = wavfile.read(filepath)\n",
    "        if len(data) > nTimeStep:\n",
    "            MajorData.append(data[0:nTimeStep])\n",
    "        elif len(data) < nTimeStep:\n",
    "            A = np.zeros(nTimeStep)\n",
    "            A[0:len(data)] = data\n",
    "            MajorData.append(A)\n",
    "        else:\n",
    "            MajorData.append(data)\n",
    "MinorData = []\n",
    "for subdir, dirs, files in os.walk(r'./Dataset/Audio_Files/Minor/'):\n",
    "    for file in files:\n",
    "        filepath = subdir + os.sep + file\n",
    "        samplerate, data = wavfile.read(filepath)\n",
    "        if len(data) > nTimeStep:\n",
    "            MinorData.append(data[0:nTimeStep])\n",
    "        elif len(data) < nTimeStep:\n",
    "            A = np.zeros(nTimeStep)\n",
    "            A[0:len(data)] = data\n",
    "            MinorData.append(A)\n",
    "        else:\n",
    "            MinorData.append(data)            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "MajorData = np.array(MajorData, dtype=np.float32)\n",
    "MinorData = np.array(MinorData, dtype=np.float32)\n",
    "# Split train and test\n",
    "def shuffle_and_split_data(X, y, seed, test_percentage, shuffle_bool=False):\n",
    "  torch.manual_seed(seed)\n",
    "  # Number of samples\n",
    "  N = X.shape[0]\n",
    "  # Shuffle data if required\n",
    "  if shuffle_bool:\n",
    "    indices =torch.randperm(N)   # Get indices to shuffle data, could use torch.randperm\n",
    "    print(indices)\n",
    "  else:\n",
    "    indices=torch.arange(N)\n",
    "    #print(indices)\n",
    "  X = X[indices]\n",
    "  y = y[indices]\n",
    "  # Split data into train/test\n",
    "  test_size =int(test_percentage*N)    # Assign test datset size using 20% of samples\n",
    "  X_test = X[:test_size]\n",
    "  y_test = y[:test_size]\n",
    "  X_train = X[test_size:]\n",
    "  y_train = y[test_size:]\n",
    "  return X_test, y_test, X_train, y_train\n",
    "MajorY = np.zeros((np.shape(MajorData)[0],2))\n",
    "MajorY[:,0] = 1\n",
    "MinorY = np.zeros((np.shape(MinorData)[0],2))\n",
    "MinorY[:,1] = 1\n",
    "testMajor, testMajorY, trainMajor, trainMajorY = shuffle_and_split_data(MajorData, MajorY, 1, 0.2)\n",
    "testMinor, testMinorY, trainMinor, trainMinorY = shuffle_and_split_data(MinorData, MinorY, 1, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainChord = np.concatenate((trainMajor, trainMinor), axis = 0)\n",
    "testChord = np.concatenate((testMajor, testMinor), axis = 0)\n",
    "trainChordY = np.concatenate((trainMajorY, trainMinorY), axis = 0)\n",
    "testChordY = np.concatenate((testMajorY, testMinorY), axis = 0)\n",
    "ds = 10\n",
    "trainChordDownSample10 = trainChord[:,0::ds]\n",
    "testChordDownSample10 = testChord[:,0::ds]\n",
    "ds = 100\n",
    "trainChordDownSample40 = trainChord[:,0::ds]\n",
    "testChordDownSample40 = testChord[:,0::ds]\n",
    "\n",
    "\n",
    "trainFFT = np.abs(np.fft.fft(trainChordDownSample10,1024))\n",
    "testFFT = np.abs(np.fft.fft(testChordDownSample10,1024))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Iteration:     1, Loss: 0.7724, Accuracy: 53.216%\n",
      "Epoch: 01, Iteration:     2, Loss: 0.7384, Accuracy: 53.216%\n",
      "Epoch: 01, Iteration:     3, Loss: 0.7857, Accuracy: 53.216%\n",
      "Epoch: 01, Iteration:     4, Loss: 0.7032, Accuracy: 53.216%\n",
      "Epoch: 01, Iteration:     5, Loss: 0.7320, Accuracy: 53.216%\n",
      "Epoch: 01, Iteration:     6, Loss: 0.7778, Accuracy: 53.216%\n",
      "Epoch: 01, Iteration:     7, Loss: 0.7662, Accuracy: 53.216%\n",
      "Epoch: 02, Iteration:     8, Loss: 0.7617, Accuracy: 53.216%\n",
      "Epoch: 02, Iteration:     9, Loss: 0.6815, Accuracy: 53.216%\n",
      "Epoch: 43, Iteration:   300, Loss: 0.7544, Accuracy: 52.632%\n",
      "Epoch: 86, Iteration:   600, Loss: 0.6738, Accuracy: 53.216%\n",
      "Epoch: 129, Iteration:   900, Loss: 0.6665, Accuracy: 49.708%\n",
      "Epoch: 172, Iteration:  1200, Loss: 0.7052, Accuracy: 49.708%\n",
      "Epoch: 215, Iteration:  1500, Loss: 0.6944, Accuracy: 52.047%\n",
      "Epoch: 258, Iteration:  1800, Loss: 0.6958, Accuracy: 50.292%\n",
      "Epoch: 300, Iteration:  2100, Loss: 0.6701, Accuracy: 50.292%\n",
      "Epoch: 343, Iteration:  2400, Loss: 0.6293, Accuracy: 54.386%\n",
      "Epoch: 386, Iteration:  2700, Loss: 0.6699, Accuracy: 55.556%\n",
      "Epoch: 429, Iteration:  3000, Loss: 0.7048, Accuracy: 55.556%\n",
      "Epoch: 472, Iteration:  3300, Loss: 0.6476, Accuracy: 55.556%\n",
      "Epoch: 515, Iteration:  3600, Loss: 0.6619, Accuracy: 55.556%\n",
      "Epoch: 558, Iteration:  3900, Loss: 0.6571, Accuracy: 54.971%\n",
      "Epoch: 600, Iteration:  4200, Loss: 0.6883, Accuracy: 57.895%\n",
      "Epoch: 643, Iteration:  4500, Loss: 0.6908, Accuracy: 57.310%\n",
      "Epoch: 686, Iteration:  4800, Loss: 0.6815, Accuracy: 57.310%\n",
      "Epoch: 729, Iteration:  5100, Loss: 0.6448, Accuracy: 57.310%\n",
      "Epoch: 772, Iteration:  5400, Loss: 0.7044, Accuracy: 57.310%\n",
      "Epoch: 815, Iteration:  5700, Loss: 0.6685, Accuracy: 57.310%\n",
      "Epoch: 858, Iteration:  6000, Loss: 0.6941, Accuracy: 57.310%\n",
      "Epoch: 900, Iteration:  6300, Loss: 0.6707, Accuracy: 57.310%\n",
      "Epoch: 943, Iteration:  6600, Loss: 0.6602, Accuracy: 57.310%\n",
      "Epoch: 986, Iteration:  6900, Loss: 0.6568, Accuracy: 57.310%\n",
      "Epoch: 1029, Iteration:  7200, Loss: 0.6810, Accuracy: 57.310%\n",
      "Epoch: 1072, Iteration:  7500, Loss: 0.6877, Accuracy: 57.310%\n",
      "Epoch: 1115, Iteration:  7800, Loss: 0.6737, Accuracy: 57.310%\n",
      "Epoch: 1158, Iteration:  8100, Loss: 0.6554, Accuracy: 57.310%\n",
      "Epoch: 1200, Iteration:  8400, Loss: 0.7044, Accuracy: 57.310%\n",
      "Epoch: 1243, Iteration:  8700, Loss: 0.6655, Accuracy: 57.310%\n",
      "Epoch: 1286, Iteration:  9000, Loss: 0.6885, Accuracy: 57.310%\n",
      "Epoch: 1329, Iteration:  9300, Loss: 0.6681, Accuracy: 57.310%\n",
      "Epoch: 1372, Iteration:  9600, Loss: 0.6305, Accuracy: 57.310%\n",
      "Epoch: 1415, Iteration:  9900, Loss: 0.6761, Accuracy: 57.895%\n",
      "Epoch: 1458, Iteration: 10200, Loss: 0.6604, Accuracy: 57.895%\n",
      "Epoch: 1500, Iteration: 10500, Loss: 0.6490, Accuracy: 57.895%\n",
      "Epoch: 1543, Iteration: 10800, Loss: 0.6797, Accuracy: 58.480%\n",
      "Epoch: 1586, Iteration: 11100, Loss: 0.6290, Accuracy: 58.480%\n",
      "Epoch: 1629, Iteration: 11400, Loss: 0.6537, Accuracy: 58.480%\n",
      "Epoch: 1672, Iteration: 11700, Loss: 0.6785, Accuracy: 58.480%\n",
      "Epoch: 1715, Iteration: 12000, Loss: 0.6635, Accuracy: 58.480%\n",
      "Epoch: 1758, Iteration: 12300, Loss: 0.6765, Accuracy: 58.480%\n",
      "Epoch: 1800, Iteration: 12600, Loss: 0.6959, Accuracy: 58.480%\n",
      "Epoch: 1843, Iteration: 12900, Loss: 0.6644, Accuracy: 58.480%\n",
      "Epoch: 1886, Iteration: 13200, Loss: 0.6719, Accuracy: 58.480%\n",
      "Epoch: 1929, Iteration: 13500, Loss: 0.6304, Accuracy: 58.480%\n",
      "Epoch: 1972, Iteration: 13800, Loss: 0.6415, Accuracy: 58.480%\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class VanillaRNN(nn.Module):\n",
    "  def __init__(self, output_size, hidden_size, input_size):\n",
    "    self.numL = 1\n",
    "    super(VanillaRNN, self).__init__()\n",
    "    self.hidden_size = hidden_size\n",
    "    self.rnn = nn.RNN(input_size, hidden_size, num_layers = self.numL)\n",
    "    self.fc = nn.Linear(self.numL*hidden_size, output_size)\n",
    "\n",
    "  def forward(self, input):\n",
    "    input = input.permute(1, 0, 2)\n",
    "    h_0 =  Variable(torch.zeros(self.numL, input.size()[1], self.hidden_size))\n",
    "    output, h_n = self.rnn(input, h_0)\n",
    "    h_n = h_n.permute(1, 0, 2)\n",
    "    h_n = h_n.contiguous().view(h_n.size()[0], h_n.size()[1]*h_n.size()[2])\n",
    "    logits = self.fc(h_n)\n",
    "    return logits\n",
    "\n",
    "## Uncomment to test\n",
    "#model = VanillaRNN(2, 5, trainFFT.shape[1])\n",
    "model = VanillaRNN(2, 5, trainChordDownSample40.shape[1])\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "num_epochs = 2000\n",
    "learning_rate = 0.0001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate, weight_decay = 0.0001)\n",
    "traindataset = torch.utils.data.TensorDataset(torch.Tensor(np.expand_dims(trainChordDownSample40,1)), torch.Tensor(trainChordY))\n",
    "testdataset = torch.utils.data.TensorDataset(torch.Tensor(np.expand_dims(testChordDownSample40,1)), torch.Tensor(testChordY))\n",
    "\n",
    "#traindataset = torch.utils.data.TensorDataset(torch.Tensor(np.expand_dims(trainFFT,1)), torch.Tensor(trainChordY))\n",
    "#testdataset = torch.utils.data.TensorDataset(torch.Tensor(np.expand_dims(testFFT,1)), torch.Tensor(testChordY))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(traindataset, batch_size = 100, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(testdataset, batch_size= 100, shuffle=False)\n",
    "count = 0\n",
    "loss_list = []\n",
    "iteration_list = []\n",
    "accuracy_list = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  for datapoints, labels in train_loader:\n",
    "    count += 1\n",
    "    model.train()\n",
    "    outputs = model(datapoints)\n",
    "    loss = loss_func(outputs, labels)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (count < 10) or not(count % 300):\n",
    "        with torch.no_grad():\n",
    "            total = 0\n",
    "            correct = 0\n",
    "            for datapoints, labels in test_loader:\n",
    "                model.eval()\n",
    "                outputs = model(datapoints)\n",
    "                predictions = torch.max(outputs, 1)[1]\n",
    "                truth = torch.max(labels, 1)[1]\n",
    "                correct += (predictions == truth).sum().numpy()\n",
    "                total += len(labels)\n",
    "            accuracy = correct * 100 / total\n",
    "            loss_list.append(loss.data)\n",
    "            iteration_list.append(count)\n",
    "            accuracy_list.append(accuracy)\n",
    "            print(f'Epoch: {epoch+1:02d}, Iteration: {count:5d}, Loss: {loss.data:.4f}, Accuracy: {accuracy:.3f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Iteration:     1, Loss: 7591.1855, Accuracy: 58.480%\n",
      "Epoch: 01, Iteration:     2, Loss: 66486.9766, Accuracy: 58.480%\n",
      "Epoch: 01, Iteration:     3, Loss: 42006.5664, Accuracy: 44.444%\n",
      "Epoch: 01, Iteration:     4, Loss: 14299.5762, Accuracy: 42.105%\n",
      "Epoch: 01, Iteration:     5, Loss: 20540.4375, Accuracy: 51.462%\n",
      "Epoch: 01, Iteration:     6, Loss: 7038.3184, Accuracy: 58.480%\n",
      "Epoch: 01, Iteration:     7, Loss: 10609.7266, Accuracy: 52.047%\n",
      "Epoch: 01, Iteration:     8, Loss: 5488.6064, Accuracy: 35.673%\n",
      "Epoch: 01, Iteration:     9, Loss: 4841.7236, Accuracy: 35.088%\n",
      "Epoch: 22, Iteration:   300, Loss: 3.9676, Accuracy: 63.158%\n",
      "Epoch: 43, Iteration:   600, Loss: 109.0068, Accuracy: 56.725%\n",
      "Epoch: 65, Iteration:   900, Loss: 32.1180, Accuracy: 60.234%\n",
      "Epoch: 86, Iteration:  1200, Loss: 5.1970, Accuracy: 60.234%\n",
      "Epoch: 108, Iteration:  1500, Loss: 40.2084, Accuracy: 62.573%\n",
      "Epoch: 129, Iteration:  1800, Loss: 14.4938, Accuracy: 63.158%\n",
      "Epoch: 150, Iteration:  2100, Loss: 335.8056, Accuracy: 57.310%\n",
      "Epoch: 172, Iteration:  2400, Loss: 0.0139, Accuracy: 60.819%\n",
      "Epoch: 193, Iteration:  2700, Loss: -0.0000, Accuracy: 60.819%\n",
      "Epoch: 215, Iteration:  3000, Loss: 0.0252, Accuracy: 60.819%\n",
      "Epoch: 236, Iteration:  3300, Loss: 0.0279, Accuracy: 60.819%\n",
      "Epoch: 258, Iteration:  3600, Loss: 0.0490, Accuracy: 60.819%\n",
      "Epoch: 279, Iteration:  3900, Loss: 0.0281, Accuracy: 60.819%\n",
      "Epoch: 300, Iteration:  4200, Loss: 0.0293, Accuracy: 60.819%\n",
      "Epoch: 322, Iteration:  4500, Loss: 0.0218, Accuracy: 60.819%\n",
      "Epoch: 343, Iteration:  4800, Loss: 0.0566, Accuracy: 60.819%\n",
      "Epoch: 365, Iteration:  5100, Loss: 0.0178, Accuracy: 60.819%\n",
      "Epoch: 386, Iteration:  5400, Loss: -0.0000, Accuracy: 60.819%\n",
      "Epoch: 408, Iteration:  5700, Loss: 0.0209, Accuracy: 60.819%\n",
      "Epoch: 429, Iteration:  6000, Loss: -0.0000, Accuracy: 60.819%\n",
      "Epoch: 450, Iteration:  6300, Loss: 0.0749, Accuracy: 60.819%\n",
      "Epoch: 472, Iteration:  6600, Loss: -0.0000, Accuracy: 60.819%\n",
      "Epoch: 493, Iteration:  6900, Loss: 0.0208, Accuracy: 60.819%\n",
      "Epoch: 515, Iteration:  7200, Loss: 0.0388, Accuracy: 60.819%\n",
      "Epoch: 536, Iteration:  7500, Loss: 0.0104, Accuracy: 60.819%\n",
      "Epoch: 558, Iteration:  7800, Loss: -0.0000, Accuracy: 60.819%\n",
      "Epoch: 579, Iteration:  8100, Loss: 0.0285, Accuracy: 60.819%\n",
      "Epoch: 600, Iteration:  8400, Loss: 0.0135, Accuracy: 60.819%\n",
      "Epoch: 622, Iteration:  8700, Loss: -0.0000, Accuracy: 60.819%\n",
      "Epoch: 643, Iteration:  9000, Loss: 0.0547, Accuracy: 60.819%\n",
      "Epoch: 665, Iteration:  9300, Loss: 0.0205, Accuracy: 60.819%\n",
      "Epoch: 686, Iteration:  9600, Loss: 0.0182, Accuracy: 60.819%\n",
      "Epoch: 708, Iteration:  9900, Loss: 0.0285, Accuracy: 60.819%\n",
      "Epoch: 729, Iteration: 10200, Loss: 0.0103, Accuracy: 60.819%\n",
      "Epoch: 750, Iteration: 10500, Loss: 0.0542, Accuracy: 60.819%\n",
      "Epoch: 772, Iteration: 10800, Loss: 0.0388, Accuracy: 60.819%\n",
      "Epoch: 793, Iteration: 11100, Loss: -0.0000, Accuracy: 60.819%\n",
      "Epoch: 815, Iteration: 11400, Loss: 0.0285, Accuracy: 60.819%\n",
      "Epoch: 836, Iteration: 11700, Loss: -0.0000, Accuracy: 60.819%\n",
      "Epoch: 858, Iteration: 12000, Loss: 0.0182, Accuracy: 60.819%\n",
      "Epoch: 879, Iteration: 12300, Loss: 0.0182, Accuracy: 60.819%\n",
      "Epoch: 900, Iteration: 12600, Loss: -0.0000, Accuracy: 60.819%\n",
      "Epoch: 922, Iteration: 12900, Loss: 0.0183, Accuracy: 60.819%\n",
      "Epoch: 943, Iteration: 13200, Loss: -0.0000, Accuracy: 60.819%\n",
      "Epoch: 965, Iteration: 13500, Loss: 0.0467, Accuracy: 60.819%\n",
      "Epoch: 986, Iteration: 13800, Loss: 0.0388, Accuracy: 60.819%\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "class Net1(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Net1, self).__init__()\n",
    "    self.hidden = nn.Linear(trainFFT.shape[1], 256)\n",
    "    self.hidden2 = nn.Linear(256, 256)\n",
    "    self.output = nn.Linear(256, 2)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.hidden(x)\n",
    "    x = F.relu(x)\n",
    "    x = self.hidden2(x)\n",
    "    x = F.relu(x)\n",
    "    x = self.output(x)\n",
    "    return x\n",
    "## Uncomment to test\n",
    "#model = VanillaRNN(2, 5, trainFFT.shape[1])\n",
    "model = Net1()\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "num_epochs = 1000\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate, weight_decay = 0.0001)\n",
    "#traindataset = torch.utils.data.TensorDataset(torch.Tensor(np.expand_dims(trainChordDownSample40,1)), torch.Tensor(trainChordY))\n",
    "#testdataset = torch.utils.data.TensorDataset(torch.Tensor(np.expand_dims(testChordDownSample40,1)), torch.Tensor(testChordY))\n",
    "\n",
    "traindataset = torch.utils.data.TensorDataset(torch.Tensor(np.expand_dims(trainFFT,1)), torch.Tensor(trainChordY))\n",
    "testdataset = torch.utils.data.TensorDataset(torch.Tensor(np.expand_dims(testFFT,1)), torch.Tensor(testChordY))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(traindataset, batch_size = 50, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(testdataset, batch_size= 50, shuffle=False)\n",
    "count = 0\n",
    "loss_list = []\n",
    "iteration_list = []\n",
    "accuracy_list = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  for datapoints, labels in train_loader:\n",
    "    datapoints = datapoints.float()\n",
    "    labels = labels.float()\n",
    "    count += 1\n",
    "    model.train()\n",
    "    outputs = model(datapoints)\n",
    "    loss = loss_func(np.squeeze(outputs), labels)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (count < 10) or not(count % 300):\n",
    "        with torch.no_grad():\n",
    "            total = 0\n",
    "            correct = 0\n",
    "            for datapoints, labels in test_loader:\n",
    "                model.eval()\n",
    "                outputs = np.squeeze(model(datapoints))\n",
    "                predictions = torch.max(outputs, 1)[1]\n",
    "                truth = torch.max(labels, 1)[1]\n",
    "                correct += (predictions == truth).sum().numpy()\n",
    "                total += len(labels)\n",
    "            accuracy = correct * 100 / total\n",
    "            loss_list.append(loss.data)\n",
    "            iteration_list.append(count)\n",
    "            accuracy_list.append(accuracy)\n",
    "            print(f'Epoch: {epoch+1:02d}, Iteration: {count:5d}, Loss: {loss.data:.4f}, Accuracy: {accuracy:.3f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = 10"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5129088cf3913680adfc03d7702067a723627da7166512a82888e31e5e87cf8b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
